{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aD7mevI6daPs"
   },
   "source": [
    "# YIELDS.IO | Stress Testing with GANs \n",
    "---------------------\n",
    "Main functions to use in order to setup and train a GAN model\n",
    "\n",
    "---------------\n",
    "- **Step 1:** Import libraries \n",
    "- **Step 2:** Implement a standalone Discriminator NN\n",
    "- **Step 3:** Implement a standalone Generator NN \n",
    "- **Step 4:** Implement a composite GAN NN\n",
    "- **Step 5:** Train the GAN NN \n",
    "- **Step 6:** Evaluate Performance\n",
    "-----------------\n",
    "#### Additional information\n",
    "- Overview of GANs: [GAN Google](https://developers.google.com/machine-learning/gan/summary)\n",
    "- Seminal GAN paper of Goodfellow et al.: [GAN Seminal paper](https://arxiv.org/abs/1406.2661)\n",
    "- Instability problems with GANs: [Stabilizing GANs](https://arxiv.org/pdf/1910.00927.pdf)\n",
    "- Wasserstein GAN to deal with vanishing gradients: [WGAN](https://arxiv.org/abs/1701.07875) \n",
    "- Improved WGAN with Gradient Penalty: [WGAN-GP](https://arxiv.org/abs/1704.00028)\n",
    "- How to add custom gradient penalty in Keras: [WGAN and Keras](https://keras.io/examples/generative/wgan_gp/) and [Change model.fit()](https://keras.io/guides/customizing_what_happens_in_fit/)\n",
    "- Multi-categorical variables and GAN: [Multi-categorical GAN](https://arxiv.org/pdf/1807.01202.pdf)\n",
    "- Alternative to WGAN: [DRAGAN](https://arxiv.org/abs/1705.07215)\n",
    "- Conditional DRAGAN on American Express dataset: [DRAGAN & American Express](https://arxiv.org/pdf/2002.02271.pdf)\n",
    "- Mulitvariate KL-divergence by KNN: [KL by KNN](https://ieeexplore.ieee.org/document/4035959)\n",
    "- Multivariate KL-divergence by  KNN p2: [KL by KNN](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.422.5121&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vz_hMfMjdaPv"
   },
   "source": [
    "## Import Libraries  \n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFiDcgFbdaPw"
   },
   "outputs": [],
   "source": [
    "# Basic libraries \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%config Completer.use_jedi = False #to increase speed of tab autocompletion (remove if obsolete/redundant)\n",
    "\n",
    "# Keras modules \n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU, ReLU\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "import keras\n",
    "\n",
    "# Tensorflow (for WGAN-GP Gradient Penalty)\n",
    "import tensorflow as tf\n",
    "\n",
    "# KNN computation used by KL-divergence function \n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01FmlUijdaP2"
   },
   "source": [
    "\n",
    "# DISCRIMINATOR: Set of Discriminators to use with GAN \n",
    "--------------------------------------------------------\n",
    "- Most relevant version as of 26/08/2020: **discriminator_setup_GENERIC**\n",
    "- All other versions are redundant and only used for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SAyWF7qdaP3"
   },
   "outputs": [],
   "source": [
    "def discriminator_setup_basic(input_shape, optimizer= Adam(lr=0.0002, beta_1=0.5)):\n",
    "    '''\n",
    "    #input_shape: the dimension of the dataset we need to classify as real or fake. \n",
    "    ## Works well with nominal data and MV Gaussian\n",
    "    '''\n",
    "\n",
    "    # Model setup (X's are hidden layers)\n",
    "    inputs= layers.Input((input_shape,))\n",
    "    X= layers.Dense(512, activation= LeakyReLU(alpha=0.2))(inputs)\n",
    "    X= layers.Dense(256, activation= LeakyReLU(alpha=0.2))(X)\n",
    "    outputs= layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    # Compile \n",
    "    model= Model(inputs, outputs)\n",
    "    model.compile(optimizer= optimizer, loss= 'binary_crossentropy', metrics= ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ng_H5-RydaP8"
   },
   "outputs": [],
   "source": [
    "def discriminator_setup_CTGAN(input_shape, optimizer= Adam(lr=0.0002, beta_1=0.5)):\n",
    "    '''\n",
    "    #input_shape: the dimension of the dataset we need to classify as real or fake. \n",
    "    ## Experimental Design CTGAN paper\n",
    "    '''\n",
    "    \n",
    "    # Model setup\n",
    "    input_layer = layers.Input((input_shape,))\n",
    "    X= layers.Dense(256, activation=LeakyReLU(alpha= 0.2))(input_layer)\n",
    "    X= Dropout(0.2)(X)\n",
    "    X= layers.Dense(256, activation=LeakyReLU(alpha= 0.2))(X)\n",
    "    X= Dropout(0.2)(X)\n",
    "    output_layer = layers.Dense(1, activation='sigmoid')(X)\n",
    "    \n",
    "    # Compile \n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.compile(optimizer= optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15o0bDzpdaQE"
   },
   "outputs": [],
   "source": [
    "def discriminator_setup_WGANGP(input_shape):\n",
    "    '''\n",
    "    #input_shape: the dimension of the dataset we need to classify as real or fake. \n",
    "    ## Experimental Design WGANGP paper\n",
    "    '''\n",
    "    \n",
    "    input_layer = layers.Input((input_shape,))\n",
    "    # Batchnormalizarion is NOT allowed (see paper for alternative if needed)\n",
    "    X= layers.Dense(256, activation=LeakyReLU(alpha= 0.2))(input_layer)\n",
    "    X= Dropout(0.2)(X)\n",
    "    X= layers.Dense(256, activation=LeakyReLU(alpha= 0.2))(X)\n",
    "    X= Dropout(0.2)(X)\n",
    "    # uses a linear activation function\n",
    "    output_layer = layers.Dense(1)(X)\n",
    "    model = Model(input_layer, output_layer)\n",
    "    \n",
    "    # no need to compile (will be done within WGAN class [see below])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5pEO3nndaQL"
   },
   "outputs": [],
   "source": [
    "def discriminator_setup_GENERIC(input_shape, numberOfNodes= 256, numberOfLayers= 2, activationFunction= 'linear'):\n",
    "    '''\n",
    "    #input_shape: the dimension of the dataset we need to classify as real or fake. \n",
    "    ## Architecture is determined by the end user \n",
    "    ## Basic setup is derived from WGAN-GP paper\n",
    "    '''\n",
    "    \n",
    "    input_layer = layers.Input((input_shape,))\n",
    "    X= input_layer\n",
    "    # Batchnormalizarion is NOT allowed (see paper for alternative if needed)\n",
    "    for eachLayer in range(numberOfLayers):\n",
    "        X= layers.Dense(numberOfNodes, activation= LeakyReLU(alpha= 0.2))(X)\n",
    "        X= Dropout(0.2)(X)\n",
    "    # WGAN-GP uses a linear activation function\n",
    "    output_layer = layers.Dense(1)(X)\n",
    "    model = Model(input_layer, output_layer)\n",
    "    \n",
    "    # no need to compile (will be done within WGAN class [see below])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXZpuIgldaQR"
   },
   "source": [
    "\n",
    "# GENERATOR: Set of Generators to use with GAN \n",
    "--------------------------------------------------------\n",
    "- Most relevant version as of 26/08/2020: **generator_setup_WGANGP_GENERIC**\n",
    "- All other versions are redundant and only used for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmhmif5HdaQT"
   },
   "outputs": [],
   "source": [
    "def generator_setup_basic(output_shape, latent_dim, activation= 'tanh'):\n",
    "    '''\n",
    "    #output_shape: the dimension of the dataset we need to generate. \n",
    "    #laten_dim: the dimension of the latent space where we draw our input for the Generator \n",
    "    #activation: the activation function used in the output layer\n",
    "    ## Works well with nominal data and MV Gaussian\n",
    "    '''\n",
    "\n",
    "    # Model setup \n",
    "    inputs= layers.Input((latent_dim,))\n",
    "    X= layers.Dense(256, activation= LeakyReLU(alpha=0.2))\n",
    "    X= BatchNormalization(momentum=0.8)(X)\n",
    "    X= layers.Dense(512, activation= LeakyReLU(alpha=0.2))\n",
    "    X= BatchNormalization(momentum=0.8)(X)    \n",
    "    X= layers.Dense(1024, activation= LeakyReLU(alpha=0.2))\n",
    "    X= BatchNormalization(momentum=0.8)(X) \n",
    "    outputs= layers.Dense(output_shape, activation= activation)\n",
    "    \n",
    "    # No need to compile model -> will be done within composite model \n",
    "    model = Model(inputs, outputs )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fzOJILEGdaQb"
   },
   "outputs": [],
   "source": [
    "def generator_setup_mixed(output_shape, latent_dim, activation_continous='tanh', activation_discrete='softmax'):\n",
    "    '''\n",
    "    #output_shape: the dimension of the dataset we need to generate. \n",
    "    #laten_dim: the dimension of the latent space where we draw our input for the Generator \n",
    "    #activation_continuous: the activation function used in the output layers connected to continuous variables\n",
    "    #activation_nominal: the activation function used in the output layers connected to nominal variables\n",
    "    ## Works well mixed distributions \n",
    "    '''\n",
    "    \n",
    "    ## Main Model ## \n",
    "    inputs= keras.Input(shape=latent_dim)\n",
    "    X= layers.Dense(256, activation= LeakyReLU(alpha=0.2), )(inputs)\n",
    "    X= BatchNormalization(momentum=0.8)(X)\n",
    "    X= layers.Dense(512, activation= LeakyReLU(alpha=0.2),)(X)\n",
    "    X= BatchNormalization(momentum=0.8)(X)\n",
    "    X= layers.Dense(1024, activation= LeakyReLU(alpha=0.2),)(X)\n",
    "    X= BatchNormalization(momentum=0.8)(X)\n",
    "    outputs= layers.Dense(output_shape)(X) \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Create Dense layers for every column of the dataset \n",
    "    # Dense layers \n",
    "    continuousColumn= layers.Dense(1, name=\"continuous1\")(outputs)\n",
    "    discreteColumn= layers.Dense(3, name='discrete1')(outputs)\n",
    "    \n",
    "    ## Create new output layers for every column of the dataset \n",
    "    continuousColumn= layers.Dense(1, activation= activation_continous, name=\"continuous2\")(continuousColumn)\n",
    "    discreteColumn= layers.Dense(3, activation= activation_discrete, name='discrete2')(discreteColumn)\n",
    "    \n",
    "    ## Concatenate all output layers together again \n",
    "    lastLayer= layers.concatenate([continuousColumn, discreteColumn])\n",
    "    modelFinal= keras.Model(inputs=inputs, outputs=lastLayer)\n",
    "    return modelFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2jfkfQCdaQg"
   },
   "outputs": [],
   "source": [
    "def generator_setup_ppnr(output_shape, latent_dim, activation_continous='tanh', activation_discrete='softmax'):\n",
    "    '''\n",
    "    #output_shape: the dimension of the dataset we need to generate. \n",
    "    #laten_dim: the dimension of the latent space where we draw our input for the Generator \n",
    "    #activation_continuous: the activation function used in the output layers connected to continuous variables\n",
    "    #activation_nominal: the activation function used in the output layers connected to nominal variables\n",
    "    ## Works well PPNR distributions \n",
    "    '''\n",
    "   \n",
    "    ## Main Model ##\n",
    "    inputs= keras.Input(shape=latent_dim)\n",
    "    X= layers.Dense(256, activation= LeakyReLU(alpha=0.2), )(inputs)\n",
    "    X= BatchNormalization(momentum=0.8)(X)\n",
    "    X= layers.Dense(512, activation= LeakyReLU(alpha=0.2),)(X)\n",
    "    X= BatchNormalization(momentum=0.8)(X)\n",
    "    X= layers.Dense(1024, activation= LeakyReLU(alpha=0.2),)(X)\n",
    "    X= BatchNormalization(momentum=0.8)(X)\n",
    "    outputs= layers.Dense(output_shape)(X) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Attach dense layers for every column of the dataset at the output of the main model in parallel \n",
    "    continuousColumns= layers.Dense(14, name=\"continuousDense\")(outputs)\n",
    "    education= layers.Dense(7, name='educationDense')(outputs)\n",
    "    marriage= layers.Dense(4, name='marriageDense')(outputs)\n",
    "    pay_1= layers.Dense(11, name='pay1Dense')(outputs)\n",
    "    pay_2= layers.Dense(10, name='pay2Dense')(outputs)\n",
    "    pay_3= layers.Dense(11, name='pay3Dense')(outputs)\n",
    "    pay_4= layers.Dense(11, name='pay4Dense')(outputs)\n",
    "    pay_5= layers.Dense(10, name='pay5Dense')(outputs)\n",
    "    pay_6= layers.Dense(10, name='pay6Dense')(outputs)\n",
    "    sex= layers.Dense(2, name='sexDense')(outputs)\n",
    "    default_payment_next_month= layers.Dense(2, name='defaultDense')(outputs)\n",
    "    \n",
    "    ## Create new output layers for every column of the dataset \n",
    "    continuousColumns= layers.Dense(14, activation= activation_continous, name=\"continuousOutput\")(continuousColumns)\n",
    "    education= layers.Dense(7, activation= activation_discrete, name='educationOutput')(education)\n",
    "    marriage= layers.Dense(4, activation= activation_discrete, name='marriageOutput')(marriage)\n",
    "    pay_1= layers.Dense(11, activation= activation_discrete, name='pay1Output')(pay_1)\n",
    "    pay_2= layers.Dense(10, activation= activation_discrete, name='pay2Output')(pay_2)\n",
    "    pay_3= layers.Dense(11, activation= activation_discrete, name='pay3Output')(pay_3)\n",
    "    pay_4= layers.Dense(11, activation= activation_discrete, name='pay4Output')(pay_4)\n",
    "    pay_5= layers.Dense(10, activation= activation_discrete, name='pay5Output')(pay_5)\n",
    "    pay_6= layers.Dense(10, activation= activation_discrete, name='pay6Output')(pay_6)\n",
    "    sex= layers.Dense(2, activation= activation_discrete, name='sexOutput')(sex)\n",
    "    default_payment_next_month= layers.Dense(2, activation= activation_discrete, name='defaultOutput')(default_payment_next_month)\n",
    "    \n",
    "    \n",
    "    ## Concatenate all output layers together again \n",
    "    lastLayer= layers.concatenate([continuousColumns, education, marriage, pay_1, pay_2, pay_3,\n",
    "                                  pay_4, pay_5, pay_6, sex, default_payment_next_month])\n",
    "    modelFinal= keras.Model(inputs=inputs, outputs=lastLayer)\n",
    "    \n",
    "    \n",
    "    return modelFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVs4g9V0daQk"
   },
   "outputs": [],
   "source": [
    "def generator_setup_CTGAN(output_shape, latent_dim, activation_continous='tanh', activation_discrete='softmax'):\n",
    "    '''\n",
    "    #output_shape: the dimension of the dataset we need to generate. \n",
    "    #laten_dim: the dimension of the latent space where we draw our input for the Generator \n",
    "    #activation_continuous: the activation function used in the output layers connected to continuous variables\n",
    "    #activation_nominal: the activation function used in the output layers connected to nominal variables\n",
    "    ## Experimental Design of CTGAN paper \n",
    "    '''\n",
    "    \n",
    "    ## Main Model ## \n",
    "    inputs= keras.Input(shape=latent_dim)\n",
    "    X= layers.Dense(256, activation= ReLU())(inputs)\n",
    "    X= BatchNormalization()(X)\n",
    "    X= layers.Dense(256, activation= ReLU())(inputs)\n",
    "    X= BatchNormalization()(X)\n",
    "    outputs= layers.Dense(output_shape)(X)\n",
    "    \n",
    "    \n",
    "    ## Create Dense layers for every column of the dataset  \n",
    "    continuousColumns= layers.Dense(14, name=\"continuousDense\")(outputs)\n",
    "    education= layers.Dense(7, name='educationDense')(outputs)\n",
    "    marriage= layers.Dense(4, name='marriageDense')(outputs)\n",
    "    pay_1= layers.Dense(11, name='pay1Dense')(outputs)\n",
    "    pay_2= layers.Dense(10, name='pay2Dense')(outputs)\n",
    "    pay_3= layers.Dense(11, name='pay3Dense')(outputs)\n",
    "    pay_4= layers.Dense(11, name='pay4Dense')(outputs)\n",
    "    pay_5= layers.Dense(10, name='pay5Dense')(outputs)\n",
    "    pay_6= layers.Dense(10, name='pay6Dense')(outputs)\n",
    "    sex= layers.Dense(2, name='sexDense')(outputs)\n",
    "    default_payment_next_month= layers.Dense(2, name='defaultDense')(outputs)\n",
    "    \n",
    "    ## Create new output layers for every column of the dataset \n",
    "    continuousColumns= layers.Dense(14, activation= activation_continous, name=\"continuousOutput\")(continuousColumns)\n",
    "    education= layers.Dense(7, activation= activation_discrete, name='educationOutput')(education)\n",
    "    marriage= layers.Dense(4, activation= activation_discrete, name='marriageOutput')(marriage)\n",
    "    pay_1= layers.Dense(11, activation= activation_discrete, name='pay1Output')(pay_1)\n",
    "    pay_2= layers.Dense(10, activation= activation_discrete, name='pay2Output')(pay_2)\n",
    "    pay_3= layers.Dense(11, activation= activation_discrete, name='pay3Output')(pay_3)\n",
    "    pay_4= layers.Dense(11, activation= activation_discrete, name='pay4Output')(pay_4)\n",
    "    pay_5= layers.Dense(10, activation= activation_discrete, name='pay5Output')(pay_5)\n",
    "    pay_6= layers.Dense(10, activation= activation_discrete, name='pay6Output')(pay_6)\n",
    "    sex= layers.Dense(2, activation= activation_discrete, name='sexOutput')(sex)\n",
    "    default_payment_next_month= layers.Dense(2, activation= activation_discrete, name='defaultOutput')(default_payment_next_month)\n",
    "    \n",
    "    \n",
    "    ## Concatenate all output layers together again \n",
    "    lastLayer= layers.concatenate([continuousColumns, education, marriage, pay_1, pay_2, pay_3,\n",
    "                                  pay_4, pay_5, pay_6, sex, default_payment_next_month])\n",
    "    modelFinal= keras.Model(inputs=inputs, outputs=lastLayer)\n",
    "    return modelFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eLBC0LztdaQq"
   },
   "outputs": [],
   "source": [
    "def generator_setup_WGANGP(output_shape, latent_dim, activation='tanh'):\n",
    "    '''\n",
    "    #output_shape: the dimension of the dataset we need to generate. \n",
    "    #laten_dim: the dimension of the latent space where we draw our input for the Generator \n",
    "    #activation_continuous: the activation function used in the output layers connected to continuous variables\n",
    "    #activation_nominal: the activation function used in the output layers connected to nominal variables\n",
    "    ## Experimental Design WGANGP Paper\n",
    "    '''\n",
    "     \n",
    "    inputs= keras.Input(shape=latent_dim)\n",
    "    X= layers.Dense(256, activation= ReLU())(inputs)\n",
    "    X= BatchNormalization()(X)\n",
    "    X= layers.Dense(256, activation= ReLU())(X)\n",
    "    X= BatchNormalization()(X)\n",
    "    # Best results are with 'tanh' function to output between [-1,1]\n",
    "    outputs= layers.Dense(output_shape, activation= activation)(X)\n",
    "    \n",
    "    # no need to compile (will be done within WGAN class [see below])\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NvfoqgPldaQw"
   },
   "outputs": [],
   "source": [
    "def generator_setup_WGANGP_PPNR(output_shape, latent_dim, activation_continous='tanh', activation_discrete='softmax'):\n",
    "    '''\n",
    "    #output_shape: the dimension of the dataset we need to generate. \n",
    "    #laten_dim: the dimension of the latent space where we draw our input for the Generator \n",
    "    #activation_continuous: the activation function used in the output layers connected to continuous variables\n",
    "    #activation_nominal: the activation function used in the output layers connected to nominal variables\n",
    "    ## Experimental Design  WGANGP Paper\n",
    "    '''\n",
    "    \n",
    "    ## Main model\n",
    "    inputs= keras.Input(shape=latent_dim)\n",
    "    X= layers.Dense(256, activation= ReLU())(inputs)\n",
    "    X= BatchNormalization()(X)\n",
    "    X= layers.Dense(256, activation= ReLU())(X)\n",
    "    X= BatchNormalization()(X)\n",
    "    outputs= layers.Dense(output_shape)(X)\n",
    "    \n",
    "    ## Create Dense layers for every column of the dataset  \n",
    "    continuousColumns= layers.Dense(14, name=\"continuousDense\")(outputs)\n",
    "    education= layers.Dense(7, name='educationDense')(outputs)\n",
    "    marriage= layers.Dense(4, name='marriageDense')(outputs)\n",
    "    pay_1= layers.Dense(11, name='pay1Dense')(outputs)\n",
    "    pay_2= layers.Dense(10, name='pay2Dense')(outputs)\n",
    "    pay_3= layers.Dense(11, name='pay3Dense')(outputs)\n",
    "    pay_4= layers.Dense(11, name='pay4Dense')(outputs)\n",
    "    pay_5= layers.Dense(10, name='pay5Dense')(outputs)\n",
    "    pay_6= layers.Dense(10, name='pay6Dense')(outputs)\n",
    "    sex= layers.Dense(2, name='sexDense')(outputs)\n",
    "    default_payment_next_month= layers.Dense(2, name='defaultDense')(outputs)\n",
    "    \n",
    "    ## Create new output layers for every column of the dataset \n",
    "    continuousColumns= layers.Dense(14, activation= activation_continous, name=\"continuousOutput\")(continuousColumns)\n",
    "    education= layers.Dense(7, activation= activation_discrete, name='educationOutput')(education)\n",
    "    marriage= layers.Dense(4, activation= activation_discrete, name='marriageOutput')(marriage)\n",
    "    pay_1= layers.Dense(11, activation= activation_discrete, name='pay1Output')(pay_1)\n",
    "    pay_2= layers.Dense(10, activation= activation_discrete, name='pay2Output')(pay_2)\n",
    "    pay_3= layers.Dense(11, activation= activation_discrete, name='pay3Output')(pay_3)\n",
    "    pay_4= layers.Dense(11, activation= activation_discrete, name='pay4Output')(pay_4)\n",
    "    pay_5= layers.Dense(10, activation= activation_discrete, name='pay5Output')(pay_5)\n",
    "    pay_6= layers.Dense(10, activation= activation_discrete, name='pay6Output')(pay_6)\n",
    "    sex= layers.Dense(2, activation= activation_discrete, name='sexOutput')(sex)\n",
    "    default_payment_next_month= layers.Dense(2, activation= activation_discrete, name='defaultOutput')(default_payment_next_month)\n",
    "    \n",
    "    \n",
    "    ## Concatenate all output layers together again \n",
    "    lastLayer= layers.concatenate([continuousColumns, education, marriage, pay_1, pay_2, pay_3,\n",
    "                                  pay_4, pay_5, pay_6, sex, default_payment_next_month])\n",
    "    modelFinal= keras.Model(inputs=inputs, outputs=lastLayer)\n",
    "    \n",
    "    # no need to compile (will be done within WGAN class [see below])    \n",
    "    return modelFinal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OtT8QF1RdaQ0"
   },
   "outputs": [],
   "source": [
    "def generator_setup_WGANGP_GENERIC(output_shape, latent_dim, numberOfNodes, numberOfLayers,  \n",
    "                                        nominalColumnsValues, nominalColumns, continuousColumns_dim,\n",
    "                                        activation_continous='tanh', activation_discrete='softmax'):\n",
    "    '''\n",
    "    #output_shape: the dimension of the dataset we need to generate. \n",
    "    #laten_dim: the dimension of the latent space where we draw our input for the Generator \n",
    "    #activation_continuous: the activation function used in the output layers connected to continuous variables\n",
    "    #activation_nominal: the activation function used in the output layers connected to nominal variables\n",
    "    ## Experimental Design  WGANGP Paper -> works as expected! \n",
    "    '''\n",
    "    \n",
    "    ## Main model\n",
    "    inputs= keras.Input(shape=latent_dim)\n",
    "    X= inputs\n",
    "    for eachLayer in range(numberOfLayers):\n",
    "        X= layers.Dense(numberOfNodes, activation= ReLU())(X)\n",
    "        X= BatchNormalization()(X)\n",
    "    outputs= layers.Dense(output_shape)(X)\n",
    "    \n",
    "    ## Create Dense layers for every column of the dataset  \n",
    "    continuousColumns= layers.Dense(continuousColumns_dim, name=\"continuousDense\")(outputs)\n",
    "    nominalColumnLayers= [layers.Dense(eachColumn, name= eachName)(outputs) for (eachColumn, eachName) \n",
    "                     in zip(nominalColumnsValues, nominalColumns)]\n",
    "    \n",
    "    ## Create new output layers for every column of the dataset \n",
    "    nominalColumnsNames2 = [eachName + '_Output' for eachName in nominalColumns]\n",
    "    continuousColumns= layers.Dense(continuousColumns_dim, activation= activation_continous, name=\"continuousOutput\")(continuousColumns)\n",
    "    nominalColumns= [layers.Dense(eachColumn, activation= activation_discrete, name= eachName)(eachLayer) \n",
    "                     for (eachColumn, eachName, eachLayer) \n",
    "                     in zip(nominalColumnsValues, nominalColumnsNames2, nominalColumnLayers)]\n",
    "    \n",
    "    ## Concatenate all output layers together again \n",
    "    lastLayer= layers.concatenate([continuousColumns, *nominalColumns])\n",
    "    modelFinal= keras.Model(inputs=inputs, outputs=lastLayer)\n",
    "    \n",
    "    # no need to compile (will be done within WGAN class [see below])    \n",
    "    return modelFinal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2f92gy0daQ6"
   },
   "source": [
    "# GAN MODEL: Composite model of Discriminator and Generator \n",
    "\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aAlTftbrdaQ7"
   },
   "source": [
    "## GAN \n",
    "--------------\n",
    "#### Define logical third-model that combines the generator and discriminator\n",
    "- sidenote line 4: Making the discriminator not trainable is a clever trick in the Keras API. The trainable property impacts the model after it is compiled. The discriminator model was compiled with trainable layers, therefore the model weights in those layers will be updated when the standalone model is updated via calls to the train_on_batch() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "foX1NpmTdaQ8"
   },
   "outputs": [],
   "source": [
    "def gan_setup(generator, discriminator):\n",
    "    '''\n",
    "    #Generator: NN Generator \n",
    "    #Discriminator: NN Discriminator\n",
    "    ## Basic GAN-model \n",
    "    '''\n",
    "    \n",
    "    # make weights in the discriminator not trainable from POV of GAN-model\n",
    "    discriminator.trainable = False\n",
    "    # connect models \n",
    "    model = Sequential()\n",
    "    # add generator \n",
    "    model.add(generator)\n",
    "    # add discriminator\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    # compile model \n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACdgMw4ZdaRB"
   },
   "source": [
    "## WGAN-GP\n",
    "-----------\n",
    "#### Use Subclassing API to override default Model.train_step() from keras library to incorporate Gradient-Penalty from WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8wAXwRrVdaRC"
   },
   "outputs": [],
   "source": [
    "class WGAN(keras.Model):\n",
    "    '''\n",
    "    Improved Wasserstein GAN model with Gradient Penalty.\n",
    "    Subclassed from the keras.Model Class\n",
    "    --------\n",
    "    Paper: Wasserstein GAN, Martin Arjovsky, Soumith Chintala, and LÂ´eon Bottou. Courant Institute of Mathematical Sciences,\n",
    "    Facebook AI Research\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, discriminator, generator, latent_dim, discriminator_extra_steps= 5, gp_weight= 10.0,):\n",
    "        # Call the parent constructor\n",
    "        super(WGAN, self).__init__()\n",
    "        # Initialize the individiual components of the model and parameters from the paper  \n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "    \n",
    "    # We need two optimizers and two loss functions (discriminator and generator)\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "    \n",
    "    # ----------------\n",
    "    # Gradient Penalty\n",
    "    # ----------------\n",
    "    \n",
    "    def gradient_penalty(self, batch_size, Xreal, Xfake):\n",
    "        \"\"\" Computes the gradient penalty.\n",
    "\n",
    "        Computes the gradient penalty of Eq.3 on a random interpolated dataset constructed \n",
    "        of the real and artificial datasets.\n",
    "        \"\"\"\n",
    "        # Compute random weighted average \n",
    "        alpha = tf.random.normal([batch_size, 1], 0.0, 1.0)\n",
    "        diff = Xfake - Xreal\n",
    "        interpolated = Xreal + alpha * diff\n",
    "        \n",
    "        # Compute the Gradient Penalty \n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Run Discriminator on interpolated dataset \n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Compute gradients wrt to the interpolated dataset \n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Compute the norm of the gradient\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis= 1))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "    \n",
    "    \n",
    "    # -----------------------\n",
    "    # Modified train_step() -> will be called during every batch update during self.fit() \n",
    "    # ------------------------\n",
    "    \n",
    "\n",
    "    def train_step(self, Xreal):\n",
    "        \n",
    "        # When calling mdl.fit(x=None, y=None, ...) x and y get converted into the tuple (x,y)\n",
    "        #so we need to separate them again \n",
    "        if isinstance(Xreal, tuple):\n",
    "            Xreal = Xreal[0]\n",
    "\n",
    "        # Extract batch size\n",
    "        batch_size = tf.shape(Xreal)[0]\n",
    "\n",
    "        ## Algorithm 1 of original paper \n",
    "        \n",
    "        #1. train generator and get the loss \n",
    "        #2. train discriminator and get the loss\n",
    "        #3. compute gradient penalty\n",
    "        #4. multiply gradient penalty with lambda \n",
    "        #5. add new gradient penalty to discriminator loss (to get eq.3)\n",
    "        #6. save generator and discriminator losses \n",
    "\n",
    "        # DISCRIMINATOR TRAINING (trained more freq. than generator)\n",
    "        \n",
    "        for i in range(self.d_steps):\n",
    "            \n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            # Gradient penalty\n",
    "            with tf.GradientTape() as tape:\n",
    "                #draw samples from generator distribution \n",
    "                Xfake = self.generator(random_latent_vectors, training=True)\n",
    "                #train discriminator on the fake and real samples respectively\n",
    "                fake_logits = self.discriminator(Xfake, training=True)\n",
    "                real_logits = self.discriminator(Xreal, training=True)\n",
    "                # Calculate discriminator loss using fake and real logits\n",
    "                d_cost = self.d_loss_fn(Xreal=real_logits, Xfake=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, Xreal, Xfake)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
    "\n",
    "        # GENERATOR TRAINING\n",
    "        \n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            #draw samples from generator distribution \n",
    "            Xfake_generator = self.generator(random_latent_vectors, training=True)\n",
    "            #evaluate samples with the discriminator \n",
    "            gen_Xfake_logits = self.discriminator(Xfake_generator, training=True)\n",
    "            #compute generator loss \n",
    "            g_loss = self.g_loss_fn(gen_Xfake_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        g_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(g_gradient, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8fqNSUVdaRI"
   },
   "source": [
    "#### Create custom callback to evaluate the KL-divergence after each epoch \n",
    "- See KLDivergence() function for more information about the KL estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uu0zkZ4odaRI"
   },
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    '''\n",
    "    Input\n",
    "    ---------\n",
    "    Xreal: sample of the true distribution \n",
    "    sample_size: size of the sample (the greater the size the slower the computation)\n",
    "    latent_dim= dimension of latent space to draw from\n",
    "    k: the kth nearest neighbor to use for the density estimation of the KL estimator\n",
    "    --------\n",
    "    Output\n",
    "    ---------- \n",
    "    kl_tracker: keeps track of the KL value after every epoch \n",
    "    ----------------\n",
    "    Custom callback that allows us to estimate the KL-divergence between the true and generated distribution \n",
    "    after every epoch when model.fit() is called. \n",
    "    --------------\n",
    "    see: https://keras.io/guides/customizing_what_happens_in_fit/ \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, Xreal, sample_size, latent_dim, k):\n",
    "        self.Xreal= Xreal\n",
    "        self.sample_size= sample_size\n",
    "        self.latent_dim= latent_dim\n",
    "        self.k= k\n",
    "        self.kl_tracker= []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        # Generate a set of real samples \n",
    "        Xreal, _= generate_real_samples(self.Xreal, self.sample_size)\n",
    "        \n",
    "        # Generate a set of fake samples from the semi-trained Generated \n",
    "        Xfake, _= generate_artificial_samples(self.model.generator, self.latent_dim, self.sample_size)\n",
    "        \n",
    "        # Compute KL-divergence with our KL-KNN estimator (see KLdivergence function for more details)\n",
    "        kl= KLdivergence(Xreal, Xfake, k= self.k, dim= 2)\n",
    "        self.kl_tracker.append(kl)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CaVKE-mSdaRL"
   },
   "source": [
    "# Training the GAN Model \n",
    "--------------------------------------------\n",
    "## Draw random samples from the real dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPXhnm24daRN"
   },
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    '''\n",
    "    #dataset: dataset to draw from \n",
    "    #n_samples: number of samples to draw \n",
    "    ## function selects random real samples to feed in batches to the discriminator (works better with the stoch grad descent)\n",
    "    '''\n",
    "    # choose random instances\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected datapoints \n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OoYeME51daRW"
   },
   "source": [
    "## Draw random samples from the Generator's latent space \n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k1oxm1ymdaRY"
   },
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    '''\n",
    "    #latent_dim: dimension of latent space \n",
    "    #n_samples: number of samples to draw \n",
    "    ## generate points in the latent space\n",
    "    '''\n",
    "    x_input = np.random.randn(latent_dim * n_samples) #generate x_input~N(0,1) for each sample \n",
    "    # reshape into a batch of inputs for the network \n",
    "    x_input = x_input.reshape(n_samples, latent_dim) #noise input dataset with n_samples and latent_dim features \n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nLbFnATMdaRf"
   },
   "source": [
    "## Generate samples from the Generator\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GpvCW7ydaRh"
   },
   "outputs": [],
   "source": [
    "def generate_artificial_samples(model, latent_dim, n_samples):\n",
    "    '''\n",
    "    #model: the Generator model to draw from \n",
    "    #latent_dim: dimension of latent space \n",
    "    #n_samples: number of samples to draw \n",
    "    ## generate artificial samples made by the Generator\n",
    "    '''\n",
    "    #generate samples from the latent space \n",
    "    gen_input= generate_latent_points(latent_dim, n_samples)\n",
    "    #convert samples to our outputspace with the Generator\n",
    "    X= model.predict(gen_input)\n",
    "    #create class 'fake' labels (0)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DmAj5u4sdaRo"
   },
   "source": [
    "## Save plot of GAN output to evaluate between epochs\n",
    "-----------\n",
    "- Auxiliary function to performance() to visually inspect some generated data samples. \n",
    "- At the moment, it makes only sense to run if data is actually respresenting something visually (e.g. An actual image; A yield Curve; Credit spread, etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRH2xDfIdaRq"
   },
   "outputs": [],
   "source": [
    "def save_plot(examples, epoch, n=10, output_directory= 'GColab'): \n",
    "    '''\n",
    "    #examples: samples of the generated data \n",
    "    #epoch: the current epoch \n",
    "    #n = sqrt() of number of examples to save (to create a square grid)\n",
    "    #output_directory= 'GColab' if we're running on Google Colab, 'desktop' if we're running from JNotebook \n",
    "    '''\n",
    "    #plot \n",
    "    for sample in range(n**2): \n",
    "        plt.subplot(n, n, sample+1)\n",
    "        plt.axis('off')\n",
    "        plt.plot(examples[sample, :])\n",
    "    \n",
    "    #Save plot\n",
    "    if output_directory == 'desktop':\n",
    "        #save plot to file\n",
    "        filename = 'generated_plot_e%03d.png' % (epoch+1)\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        \n",
    "    if output_directory == 'GColab':\n",
    "        #save plot to file\n",
    "        filename = '/content/gdrive/My Drive/Stress Testing with GANs/Images/generated_plot_e%03d.png' % (epoch+1)\n",
    "        plt.savefig(filename)    \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ge3EQNeedaRw"
   },
   "source": [
    "## Keep track of performance during training \n",
    "-----------\n",
    "#### Evaluate the performance of the discriminator based on accuracy on the 'real' and 'fake' samples  and save the corresponding generator model \n",
    "- The function will be called during training between running epochs so we ex-post evaluate when the optimal stopping epoch is reached\n",
    "- Only works with Vanilla GAN setup [i.e. gan_setup()]\n",
    "- WGAN-GP is subclassed from the keras.Model class so we need a different way of tracking performance -> See custom callback function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KExnJO7ldaRx"
   },
   "outputs": [],
   "source": [
    "def performance(epoch, g_mdl, d_mdl, dataset, latent_dim, n_samples= 100, output_directory = 'GColab'):\n",
    "    '''\n",
    "    #epoch: the current epoch \n",
    "    #g_mdl: generator \n",
    "    #d_mdl: discriminator \n",
    "    #dataset: the original dataset \n",
    "    #latent_dim: dimension of latent space\n",
    "    #n_samples: number of samples to draw from the real and artificial dataset \n",
    "    #output_directory= 'GColab' if we're running on Google Colab, 'desktop' if we're running from JNotebook \n",
    "    ## Returns accuracy of Discriminator over real and fake samples (in tuple form)\n",
    "    '''\n",
    "    \n",
    "    #generate 'real' samples\n",
    "    Xreal, Yreal = generate_real_samples(dataset, n_samples)\n",
    "    #evaluate discriminator on 'real' samples \n",
    "    _, acc_real = d_mdl.evaluate(Xreal, Yreal, verbose= 0)\n",
    "    \n",
    "    #generate 'fake' samples\n",
    "    Xfake, Yfake = generate_artificial_samples(g_mdl, latent_dim, n_samples)\n",
    "    #evaluate discriminator on 'fake' samples \n",
    "    _, acc_fake = d_mdl.evaluate(Xfake, Yfake, verbose= 0)\n",
    "    \n",
    "    #print performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    \n",
    "    #save Generator \n",
    "    \n",
    "    if output_directory == 'desktop':\n",
    "        filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "        g_mdl.save(filename)\n",
    "    \n",
    "    #save Generator GColab ####\n",
    "    if output_directory == 'GColab':\n",
    "        filename = '/content/gdrive/My Drive/Stress Testing with GANs/Images/generator_model_%03d.h5' % (epoch+1)\n",
    "        g_mdl.save(filename)\n",
    "    \n",
    "    #save plot\n",
    "    save_plot(Xfake, epoch, 10, output_directory)\n",
    "    \n",
    "    return acc_real, acc_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jG36YuYFdaR2"
   },
   "source": [
    "## Track performance visually \n",
    "-----------\n",
    "- Auxiliary function to train_gan() to track performance during training phase. \n",
    "- Only works with Vanilla GAN setup [i.e. gan_setup()]\n",
    "- WGAN-GP is subclassed from the keras.Model class so we need a different way of tracking performance -> see plot_metrics() function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VFbHcD4EdaR2"
   },
   "outputs": [],
   "source": [
    "def plot_loss(d_loss, g_loss, d_real_acc, d_fake_acc):\n",
    "    fig= plt.figure(figsize=[10,5])\n",
    "    # Loss plots\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(d_loss, label= 'Discriminator')\n",
    "    plt.plot(g_loss, label= 'Generator')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Functions during Training')\n",
    "    \n",
    "    # Accuracy plots\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(d_real_acc, label= 'Real samples')\n",
    "    plt.plot(d_fake_acc, label= 'Fake samples')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy of the Discriminator During Training')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wyFNfaPlfk4"
   },
   "source": [
    "### WGAN-GP version to track performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s7BYkhRalYQF"
   },
   "outputs": [],
   "source": [
    "# Plots the loss functions of Discriminator and Generator and the KL-divergence obtained during training \n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "\n",
    "## Main function\n",
    "\n",
    "def plot_metrics(history, cbk, zoom_in= True):\n",
    "  '''\n",
    "  INPUT\n",
    "  --------\n",
    "  history: Dictionary containing losses obtained after running wgan.fit()\n",
    "  cbk: callback obtained after running wgn.fit()\n",
    "  \n",
    "  OUTPUT\n",
    "  ---------\n",
    "  plots the losses and Kullback-Leibler divergence during training\n",
    "  '''\n",
    "\n",
    "  g_loss= history.history['g_loss']\n",
    "  d_loss= history.history['d_loss']\n",
    "  kl= np.array(cbk.kl_tracker)\n",
    "\n",
    "  #function to set our spines invisible \n",
    "  def make_patch_spines_invisible(ax):\n",
    "      ax.set_frame_on(True)\n",
    "      ax.patch.set_visible(False)\n",
    "      for sp in ax.spines.values():\n",
    "          sp.set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "  # ------------------        \n",
    "  ## Main Graph \n",
    "  # ------------------\n",
    "\n",
    "\n",
    "  #setup figure \n",
    "  fig, host = plt.subplots(figsize=[20, 5])\n",
    "  fig.subplots_adjust(right=0.75)\n",
    "  par1 = host.twinx()\n",
    "  par2 = host.twinx()\n",
    "  # Offset the right spine of par2.  The ticks and label have already been\n",
    "  # placed on the right by twinx above.\n",
    "  par2.spines[\"right\"].set_position((\"axes\", 1.1))\n",
    "  # Having been created by twinx, par2 has its frame off, so the line of its\n",
    "  # detached spine is invisible.  First, activate the frame but make the patch\n",
    "  # and spines invisible.\n",
    "  make_patch_spines_invisible(par2)\n",
    "  # Second, show the right spine.\n",
    "  par2.spines[\"right\"].set_visible(True)\n",
    "  # Ready to plot our graphs \n",
    "  epochs = range(1, len(g_loss) + 1)\n",
    "  p1, = host.plot(epochs, kl, 'b', label=\"KL-Divergence\")\n",
    "  p2, = par1.plot(d_loss, 'r', label=\"d_loss\")\n",
    "  p3, = par2.plot(g_loss, 'g', label=\"g_loss\")\n",
    "  # Name labels appropriately \n",
    "  host.set_xlabel(\"Epochs\", fontweight='bold')\n",
    "  host.set_ylabel(\"KL-Divergence\", fontweight='bold')\n",
    "  par1.set_ylabel(\"d_loss\", fontweight='bold')\n",
    "  par2.set_ylabel(\"g_loss\", fontweight='bold')\n",
    "  # Keep coloring uniform across the entire figure \n",
    "  host.yaxis.label.set_color(p1.get_color())\n",
    "  par1.yaxis.label.set_color(p2.get_color())\n",
    "  par2.yaxis.label.set_color(p3.get_color())\n",
    "  # Clean up ticks \n",
    "  tkw = dict(size=4, width=1.5)\n",
    "  host.tick_params(axis='y', colors=p1.get_color(), **tkw)\n",
    "  par1.tick_params(axis='y', colors=p2.get_color(), **tkw)\n",
    "  par2.tick_params(axis='y', colors=p3.get_color(), **tkw)\n",
    "  host.tick_params(axis='x', **tkw)\n",
    "  # Add outside legend and give title\n",
    "  lines = [p1, p2, p3]\n",
    "  host.legend(lines, [l.get_label() for l in lines], title= 'LEGEND', bbox_to_anchor=(1.30, 1),)\n",
    "  plt.title('Generator/Discriminator Loss functions and the KL-divergence', fontweight='bold')\n",
    "\n",
    "\n",
    "  # ------------------------------\n",
    "  ## Zoom-ins to get more detail \n",
    "  # ------------------------------\n",
    "\n",
    "  if zoom_in == True:\n",
    "\n",
    "    # Zoom into a part of KL-divergence graph\n",
    "    axins = zoomed_inset_axes(host, 2, loc= 5) # (graph, zoom-in factor, location on graph to display the zoom)\n",
    "    #What to zoom in on\n",
    "    axins.plot(epochs, kl)\n",
    "    #limits \n",
    "    x1= len(kl) - int(np.round(0.1*len(kl)))\n",
    "    x2= len(kl)\n",
    "    y1= np.min(kl[x1: x2])\n",
    "    y2= np.max(kl[x1: x2])\n",
    "    axins.set_xlim(x1, x2) # apply the x-limits\n",
    "    axins.set_ylim(y1, y2) # apply the y-limits\n",
    "    #Add connecting lines to original plot \n",
    "    mark_inset(host, axins, loc1=3, loc2=4, fc=\"none\", ec=\"0\", )\n",
    "\n",
    "    #Zoom into a part of the Discriminator loss graph\n",
    "    axins = zoomed_inset_axes(par1, 2, loc= 10) # (graph, zoom-in factor, location on graph to display the zoom)\n",
    "    #What to zoom in on\n",
    "    axins.plot(epochs, d_loss, 'r')\n",
    "    #limits \n",
    "    x1= len(d_loss) - int(np.round(0.1*len(d_loss)))\n",
    "    x2= len(d_loss)\n",
    "    y1= np.min(d_loss[x1: x2])\n",
    "    y2= np.max(d_loss[x1: x2])\n",
    "    axins.set_xlim(x1, x2) # apply the x-limits\n",
    "    axins.set_ylim(y1, y2) # apply the y-limits\n",
    "    #Add connecting lines to original plot \n",
    "    mark_inset(par1, axins, loc1=2, loc2=4, fc=\"None\", ec=\"0\", )\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYNRE2IjdaR7"
   },
   "source": [
    "## Training the GAN Model \n",
    "-------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmcgrQTpdaR8"
   },
   "outputs": [],
   "source": [
    "def train_gan(d_mdl, g_mdl, gan_mdl, dataset, latent_dim, n_epochs= 100, batch_size= 10000\n",
    "          , plotLoss= False, output_directory= 'GColab'):\n",
    "    '''\n",
    "    #d_mdl: discriminator \n",
    "    #g_mdl: generator \n",
    "    #gan_mdl: composite model of discriminator and generator \n",
    "    #latent_dim: dimension of latent space\n",
    "    #dataset: the original dataset \n",
    "    #n_epochs: number of epochs (which determines the number of updates to the generator)\n",
    "    #batch_size: batch size (which determines the number of updates to the discriminator)\n",
    "    '''\n",
    "    \n",
    "    numberOfBatches = int(dataset.shape[0] / batch_size) #number of batches in one epoch \n",
    "    half_batch = int(batch_size / 2)\n",
    "    #initialize lists to track performance over training batches\n",
    "    d_loss_track, g_loss_track, d_real_acc, d_fake_acc= list(), list(), list(), list() \n",
    "    #loop through epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        #loop through batches:\n",
    "        for batch in range(numberOfBatches):\n",
    "            \n",
    "            # ----------------\n",
    "            # DISCRIMINATOR\n",
    "            # ----------------\n",
    "            \n",
    "            #generate random 'real' samples\n",
    "            Xreal, Yreal= generate_real_samples(dataset= dataset, n_samples= half_batch)\n",
    "            #generate random 'fake' samples \n",
    "            Xfake, Yfake= generate_artificial_samples(g_mdl,latent_dim, half_batch)\n",
    "            #combine 'real' and 'fake' samples into one training set for the discriminator to train on \n",
    "            X, y = np.vstack((Xreal, Xfake)), np.vstack((Yreal, Yfake))\n",
    "            #update discriminator \n",
    "            d_loss, d_acc = d_mdl.train_on_batch(X, y)\n",
    "            #track loss function\n",
    "            d_loss_track.append(d_loss)\n",
    "            \n",
    "            # --------------\n",
    "            # GENERATOR \n",
    "            # --------------\n",
    "            \n",
    "            #generate latent space \n",
    "            Xgan= generate_latent_points(latent_dim, batch_size)\n",
    "            #create INVERTED labels (0 -> 1)\n",
    "            yGan= np.ones((batch_size, 1))\n",
    "            #update generator (based on discriminator errors)\n",
    "            g_loss = gan_mdl.train_on_batch(Xgan, yGan)\n",
    "            #track loss function\n",
    "            g_loss_track.append(g_loss)\n",
    "            #summarize performance on this batch \n",
    "            print(\"LOSS:\")\n",
    "            print('>%d, %d/%d, d=%.3f, g=%.3f' % (epoch+1, batch+1, numberOfBatches, d_loss, g_loss))\n",
    "            print(\"----------------------\")\n",
    "            \n",
    "        # evaluate the model performance, sometimes\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            #print(\"LOSS:\")\n",
    "            #print('>%d, %d/%d, d=%.3f, g=%.3f' % (epoch+1, batch+1, numberOfBatches, d_loss, g_loss))\n",
    "            #print(\"----------------------\")\n",
    "            performance(epoch, g_mdl, d_mdl, dataset, latent_dim, 100, output_directory)\n",
    "            \n",
    "            # Track accuracy of Discriminator \n",
    "            #generate 'real' samples\n",
    "            Xreal, Yreal = generate_real_samples(dataset, 1000)\n",
    "            #evaluate discriminator on 'real' samples \n",
    "            _, acc_real = d_mdl.evaluate(Xreal, Yreal, verbose= 0)\n",
    "            #generate 'fake' samples\n",
    "            Xfake, Yfake = generate_artificial_samples(g_mdl, latent_dim, 1000)\n",
    "            #evaluate discriminator on 'fake' samples \n",
    "            _, acc_fake = d_mdl.evaluate(Xfake, Yfake, verbose= 0)\n",
    "            # save performance\n",
    "            d_real_acc.append(acc_real)\n",
    "            d_fake_acc.append(acc_fake)\n",
    "            \n",
    "    if plotLoss == True:\n",
    "        plot_loss(d_loss_track, g_loss_track, d_real_acc, d_fake_acc)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZzL395FpIgz"
   },
   "source": [
    "## Training the WGAN-GP Model \n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5HtXwjFpHn-"
   },
   "outputs": [],
   "source": [
    "def compile_wgan(dataset_dim, dataset_nominal_dim, dataset_continuous_dim, nominalColumns,\n",
    "                 epochs= 1000, noise_dim= 512, batch_size= 512, \n",
    "                 discriminator_steps= 5, sample_size_kl= 1000, knn= 10):\n",
    "\n",
    "  ## Optimizer for both the networks\n",
    "  #learning_rate=0.0002, beta_1=0.5 are recommended (keep hardcoded for now)\n",
    "  generator_optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n",
    "  discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "\n",
    "  ## Define the loss function to be used for the Discrimiator\n",
    "  # Gradient Penalty is added when calling WGAN.fit()\n",
    "  def discriminator_loss(Xreal, Xfake):\n",
    "      real_loss = tf.reduce_mean(Xreal)\n",
    "      fake_loss = tf.reduce_mean(Xfake)\n",
    "      return fake_loss - real_loss\n",
    "\n",
    "  ## Define the loss functions to be used for the Generator\n",
    "  def generator_loss(Xfake):\n",
    "      return -tf.reduce_mean(Xfake)\n",
    "\n",
    "  ## MODEL \n",
    "  d_model = discriminator_setup_WGANGP(dataset_dim)\n",
    "  g_model = generator_setup_WGANGP_GENERIC(output_shape= dataset_dim, latent_dim= noise_dim, \n",
    "                                           numberOfNodes= 256, numberOfLayers= 2, nominalColumnsValues= dataset_nominal_dim, \n",
    "                                           nominalColumns= nominalColumns, continuousColumns_dim= dataset_continuous_dim,\n",
    "                                           activation_continous='tanh', activation_discrete='softmax')\n",
    "  # Get the wgan model\n",
    "  wgan = WGAN(discriminator=d_model, generator=g_model, latent_dim=noise_dim, discriminator_extra_steps= discriminator_steps)\n",
    "  # Compile the wgan model\n",
    "  wgan.compile(d_optimizer=discriminator_optimizer, g_optimizer=generator_optimizer, g_loss_fn=generator_loss, d_loss_fn=discriminator_loss)\n",
    "\n",
    "  return d_model, g_model, wgan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s31KvHe1daR_"
   },
   "source": [
    "# EVALUATING THE MODEL \n",
    "----------------\n",
    "## Kullback Leibler Divergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fBRX-2mWdaSB"
   },
   "outputs": [],
   "source": [
    "def KLdivergence(Xreal, Xfake, k=1, dim= 2):\n",
    "    \"\"\"\n",
    "    Kullback-Leibler estimator in a multivariate universe.\n",
    "\n",
    "    #x : dataset sampled from distribution P()\n",
    "    #y : dataset sampled from distribution Q() [from the same multivariate universe]\n",
    "    #dim: number of dimensions \n",
    "    #output : Estimate of D(P||Q) based on KNN density estimates\n",
    "\n",
    "    #see: \n",
    "    - PÃ©rez-Cruz, F. Kullback-Leibler divergence estimation of\n",
    "    continuous distributions\n",
    "    - Qing Wang, Sanjeev R. Kulkarni, Sergio VerdÂ´u, \n",
    "    A Nearest-Neighbor Approach to Estimating Divergence between Continuous Random Vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parameters \n",
    "    \n",
    "    #add second dimension in case of vectors \n",
    "    if dim == 1:\n",
    "        Xreal= Xreal.reshape(-1,1)\n",
    "        Xfake= Xfake.reshape(-1,1)\n",
    "        \n",
    "    n,d = Xreal.shape\n",
    "    m,dy = Xfake.shape\n",
    "    \n",
    "    # K Nearest Neighbors [KD tree to speed up process, but results in possible mistakes (!)]\n",
    "    XrealTree = cKDTree(Xreal)\n",
    "    Xfaketree = cKDTree(Xfake)\n",
    "\n",
    "    # Get (euclidian) distance to closest neighbors (k= 1)\n",
    "    if k == 1:\n",
    "        r = XrealTree.query(Xreal, k=2)[0][:,1] #1st \"neighbor\" is x itself, so we look at 2nd neighbor\n",
    "        s = Xfaketree.query(Xreal, k=1)[0]\n",
    "    else:\n",
    "        r = XrealTree.query(Xreal, k= k+1)[0][:,-1] #1st \"neighbor\" is x itself, so we look at 2nd neighbor\n",
    "        s = Xfaketree.query(Xreal, k= k)[0][:,-1]\n",
    "    \n",
    "    # Addendum Paper PÃ©rez-Cruz et al. Eq.14: add negative sign to first right-hand side term (see Qing Wang et al.)\n",
    "    return -np.log(r/s).sum() * d / n + np.log(m / (n - 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUouOI00daSG"
   },
   "source": [
    "## Test for Normality (To be used when original dataset is drawn from N~(mu, cov) )\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_-VoYHCdaSH"
   },
   "outputs": [],
   "source": [
    "def Normality(Xreal, Xfake, multivariate_test= 'False', plot= True, dim_plot= -1): \n",
    "    '''\n",
    "    #Xreal: samples from the real dataset \n",
    "    #Xfake: samples from the artificial dataset\n",
    "    #multivariate_test: Henze-Zirkler multivariate normality test of the pingouin library \n",
    "    (test requires sign. memory allocation!)\n",
    "    #plot: plots the distributions of the real and generated dataset \n",
    "    #dim_plot: which dimension to plot \n",
    "    ## Compare distribution structures between the generated data and the real data \n",
    "    '''\n",
    "    from scipy.stats import normaltest, shapiro\n",
    "    from scipy.stats import skew, kurtosis\n",
    "    \n",
    "    # Compare mean vector \n",
    "    mean_real = np.round(np.mean(Xreal, axis= 0), 2)\n",
    "    mean_fake = np.round(np.mean(Xfake, axis= 0), 2)\n",
    "    # Compare first Covariance matrix \n",
    "    corr_real= np.round(np.corrcoef(Xreal, rowvar= False), 2)\n",
    "    corr_fake= np.round(np.corrcoef(Xfake, rowvar= False), 2)\n",
    "    corr_diff= corr_real - corr_fake\n",
    "    # Compare Skewness \n",
    "    skew_real= np.round(skew(Xreal, axis= 0), 2)\n",
    "    skew_fake= np.round(skew(Xfake, axis= 0), 2)\n",
    "    # Compare Kurtosis \n",
    "    kurtosis_real= np.round(kurtosis(Xreal, axis= 0), 2)\n",
    "    kurtosis_fake= np.round(kurtosis(Xfake, axis= 0), 2)\n",
    "    print('REAl: mu', mean_real)\n",
    "    print('FAKE: mu', mean_fake)\n",
    "    print('--------')\n",
    "    print('REAl: cov', corr_real)\n",
    "    print('FAKE: cov', corr_fake)\n",
    "    print('Difference: cov', corr_diff)\n",
    "    print('--------')\n",
    "    print('REAl: skew', skew_real)\n",
    "    print('FAKE: skew', skew_fake)\n",
    "    print('--------')\n",
    "    print('REAl: kurt', kurtosis_real)\n",
    "    print('FAKE: kurt', kurtosis_fake)\n",
    "    print('--------')\n",
    "    \n",
    "    # Test for normalitiy \n",
    "    if multivariate_test == True:\n",
    "        from pingouin import multivariate_normality\n",
    "        multivariate_normality(dataset_Gaussian)\n",
    "    # Plot distribution (from last variable)\n",
    "    if plot == True:\n",
    "        fig = plt.figure(figsize=[10,5])\n",
    "        plt.hist(Xreal[:, dim_plot], bins= 100, label= 'Real Distribution');\n",
    "        plt.hist(Xfake[:, dim_plot], bins= 100, label= 'Generated Distribution');\n",
    "        plt.legend()\n",
    "        plt.title('Real vs. Generated Distribution')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aaRVRjCSnAQm"
   },
   "source": [
    "# Processing the data \n",
    "--------------------\n",
    "- Functions used to pre- and postprocess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpdnrOp6nJaX"
   },
   "outputs": [],
   "source": [
    "# Transforms ohe dataset to list of individual ohe variables \n",
    "def variableSeparator(nominalColumnsValues, nominalDataset):\n",
    "    '''\n",
    "    INPUT\n",
    "    ------------\n",
    "    nominalColumnsValues: A list containing the number of unique values of each variable \n",
    "    nominalDataset: a dataset containing one-hot encoded nominal variables \n",
    "    ---------------\n",
    "    OUTPUT\n",
    "    ----------------\n",
    "    list with the i-th element containing all of the one-hot encoded columns of the i-th nominal variable  \n",
    "    ----------------\n",
    "    Separates all of the individual nominal variables into a list, so it's easy to perform computations on each variable\n",
    "    '''\n",
    "    #initialize loop\n",
    "    dim= len(nominalColumnsValues)\n",
    "    variablesSeparated= []\n",
    "    #loop through each variable \n",
    "    for eachVariable in range(dim):\n",
    "        #special case when starting off the loop where we can't use the idx variable yet\n",
    "        if eachVariable == 0:\n",
    "            tmp= nominalDataset[:, eachVariable:eachVariable+nominalColumnsValues[eachVariable]]\n",
    "            variablesSeparated.append(tmp)\n",
    "            idx= eachVariable+nominalColumnsValues[eachVariable]\n",
    "        #extract the all of one-hot encoded columns attached to the individual variable \n",
    "        else: \n",
    "            tmp= nominalDataset[:, idx:idx+nominalColumnsValues[eachVariable]]\n",
    "            variablesSeparated.append(tmp)\n",
    "            idx+= nominalColumnsValues[eachVariable]\n",
    "            \n",
    "    return variablesSeparated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qY87oONzWPic"
   },
   "outputs": [],
   "source": [
    "# Tranforms probabilities to ohe format (used in postprocessing)\n",
    "def retransformer(nominalVariable): \n",
    "  '''\n",
    "  INPUT\n",
    "  -----------\n",
    "  nominalVariable: probability matrix linked to the ohe matrix of a nomnial variable\n",
    "  ------------\n",
    "  OUTPUT\n",
    "  ----------\n",
    "  ohe matrix of the nominal variable\n",
    "  -------------------------\n",
    "  Function used in the postprocessing phase to go back from ohe probabilities to simple ohe.  \n",
    "\n",
    "  '''\n",
    "  idx = nominalVariable.argmax(axis=1)\n",
    "  out = np.zeros_like(nominalVariable,dtype=float)\n",
    "  out[np.arange(nominalVariable.shape[0]), idx] = 1\n",
    "  return out"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GAN_functions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
